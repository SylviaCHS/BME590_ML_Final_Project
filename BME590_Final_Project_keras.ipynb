{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_feature(folder, num):\n",
    "    filename = glob.glob(os.path.join(folder, '*'))\n",
    "    img_arr = np.zeros([len(filename), 100, 100, 3])\n",
    "    label = num * np.ones(len(filename), dtype=\"float32\")\n",
    "    for i, name in enumerate(filename):\n",
    "        img = Image.open(name)\n",
    "        img_arr[i, :, :, :] = np.asarray(img, dtype=\"uint8\")\n",
    "    return img_arr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8076, 100, 100, 3)\n",
      "(8076,)\n",
      "Training data shape (7268, 100, 100, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tb_img_arr, tb_label = read_feature('./TB_Image', 1)\n",
    "non_tb_img_arr, non_tb_label = read_feature('./Non-TB_Image', 0)\n",
    "images = np.concatenate((tb_img_arr, non_tb_img_arr))\n",
    "labels = np.concatenate((tb_label, non_tb_label))\n",
    "\n",
    "print(np.shape(images))\n",
    "print(np.shape(labels))\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.1)\n",
    "\n",
    "X_train = X_train.astype(np.int)\n",
    "X_val = X_val.astype(np.int)\n",
    "y_train = y_train.astype(np.int)\n",
    "y_val = y_val.astype(np.int)\n",
    "\n",
    "# change into one-hot vector\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 2) \n",
    "y_val = tf.keras.utils.to_categorical(y_val, 2)\n",
    "\n",
    "# reshape dataset\n",
    "X_train = X_train.reshape(X_train.shape[0], 100, 100, 3)\n",
    "X_val = X_val.reshape(X_val.shape[0], 100, 100, 3)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('Training data shape', X_train.shape)\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(X_train[0].reshape(100, 100, 3), cmap=plt.cm.Greys);\n",
    "ax2.imshow(X_train[1].reshape(100, 100, 3), cmap=plt.cm.Greys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(model):\n",
    "    loss = []\n",
    "    acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3)\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs/{}'.format('model_name'))\n",
    "    hist = model.fit(X_train, y_train,\n",
    "                     batch_size=64,\n",
    "                     epochs=50,  # Run thru all the data point in each epoch\n",
    "                     verbose=1,\n",
    "                     validation_data=(X_val, y_val),\n",
    "                     #callbacks=[tensorboard])\n",
    "                     callbacks=[early_stop, tensorboard])\n",
    "    #val_err.append(hist.history['val_mean_absolute_error'][-1]) # a dict\n",
    "    loss.append(hist.history['loss'][-1])\n",
    "    val_loss.append(hist.history['val_loss'][-1])\n",
    "    acc.append(hist.history['acc'][-1])\n",
    "    val_acc.append(hist.history['val_acc'][-1])   \n",
    "    \n",
    "    return loss, val_loss, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a VGG network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG(activ):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=activ, input_shape=(100, 100, 3)),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(128, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(256, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.Conv2D(256, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation=activ),\n",
    "        tf.keras.layers.Dense(4096, activation=activ),\n",
    "        tf.keras.layers.Dense(1000, activation=activ),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    param = model.count_params()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.000001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "   \n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnnmodel(n, activ, param):\n",
    "    model = tf.keras.Sequential([])\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(100, 100, 3)))\n",
    "    for i in range(n):\n",
    "        model.add(tf.keras.layers.Dense(100, activation=activ))\n",
    "    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "    # model.summary()\n",
    "    # model.count_params()\n",
    "    param.append(model.count_params())\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.000001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', 'mae'])\n",
    "    return model, param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning with VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG with activation \"relu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 13, 13, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 63,659,322\n",
      "Trainable params: 63,659,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7268 samples, validate on 808 samples\n",
      "Epoch 1/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.6599 - acc: 0.6076 - val_loss: 0.6479 - val_acc: 0.5718\n",
      "Epoch 2/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.6044 - acc: 0.7025 - val_loss: 0.5896 - val_acc: 0.6968\n",
      "Epoch 3/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.5668 - acc: 0.7367 - val_loss: 0.5489 - val_acc: 0.7302\n",
      "Epoch 4/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.5183 - acc: 0.7931 - val_loss: 0.4977 - val_acc: 0.8181\n",
      "Epoch 5/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.4703 - acc: 0.8261 - val_loss: 0.4609 - val_acc: 0.8589\n",
      "Epoch 6/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.4149 - acc: 0.8664 - val_loss: 0.3979 - val_acc: 0.8824\n",
      "Epoch 7/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.3753 - acc: 0.8773 - val_loss: 0.3503 - val_acc: 0.9097\n",
      "Epoch 8/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.3259 - acc: 0.9045 - val_loss: 0.3048 - val_acc: 0.9059\n",
      "Epoch 9/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2839 - acc: 0.9232 - val_loss: 0.2838 - val_acc: 0.9282\n",
      "Epoch 10/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2553 - acc: 0.9300 - val_loss: 0.2464 - val_acc: 0.9220\n",
      "Epoch 11/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2335 - acc: 0.9353 - val_loss: 0.2219 - val_acc: 0.9319\n",
      "Epoch 12/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2172 - acc: 0.9389 - val_loss: 0.2168 - val_acc: 0.9356\n",
      "Epoch 13/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2019 - acc: 0.9419 - val_loss: 0.2060 - val_acc: 0.9196\n",
      "Epoch 14/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1981 - acc: 0.9393 - val_loss: 0.1903 - val_acc: 0.9431\n",
      "Epoch 15/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1869 - acc: 0.9439 - val_loss: 0.1910 - val_acc: 0.9455\n",
      "Epoch 16/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1779 - acc: 0.9479 - val_loss: 0.1744 - val_acc: 0.9455\n",
      "Epoch 17/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1776 - acc: 0.9455 - val_loss: 0.1698 - val_acc: 0.9480\n",
      "Epoch 18/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1690 - acc: 0.9490 - val_loss: 0.1787 - val_acc: 0.9319\n",
      "Epoch 19/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1642 - acc: 0.9507 - val_loss: 0.1622 - val_acc: 0.9505\n",
      "Epoch 20/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1620 - acc: 0.9512 - val_loss: 0.1758 - val_acc: 0.9517\n",
      "Epoch 21/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1572 - acc: 0.9535 - val_loss: 0.1708 - val_acc: 0.9381\n",
      "Epoch 22/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1555 - acc: 0.9539 - val_loss: 0.1553 - val_acc: 0.9567\n",
      "Epoch 23/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1497 - acc: 0.9547 - val_loss: 0.1624 - val_acc: 0.9530\n",
      "Epoch 24/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1587 - acc: 0.9490 - val_loss: 0.1952 - val_acc: 0.9245\n",
      "Epoch 25/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1666 - acc: 0.9491 - val_loss: 0.1522 - val_acc: 0.9554\n",
      "Epoch 26/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1465 - acc: 0.9545 - val_loss: 0.1495 - val_acc: 0.9542\n",
      "Epoch 27/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1471 - acc: 0.9564 - val_loss: 0.1559 - val_acc: 0.9480\n",
      "Epoch 28/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1468 - acc: 0.9546 - val_loss: 0.1708 - val_acc: 0.9530\n",
      "Epoch 29/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1397 - acc: 0.9580 - val_loss: 0.1759 - val_acc: 0.9505\n"
     ]
    }
   ],
   "source": [
    "activ = 'relu'\n",
    "model_VGG1, param_VGG1 = VGG(activ)\n",
    "loss_VGG1, val_loss_VGG1, hist_VGG1= train_data(model_VGG1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_and_loss(hist):\n",
    "    acc = hist['acc']\n",
    "    loss = hist['loss']\n",
    "    val_acc = hist['val_acc']\n",
    "    val_loss = hist['val_loss']\n",
    "    \n",
    "    plt.plot(acc, 'r-o')\n",
    "    plt.title(\"Trainning accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(loss, 'g-o')\n",
    "    plt.title(\"Trainning loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(val_acc, 'b-o')\n",
    "    plt.title(\"Validation accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(val_loss, 'm-o')\n",
    "    plt.title(\"Validation loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_and_loss(hist_VGG1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for statistics calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = model.predict(X_val)\n",
    "y_val = np.argmax(y_val, axis=-1)\n",
    "predictions = np.argmax(predictions, axis=-1)\n",
    "c = confusion_matrix(y_val, predictions)\n",
    "print('Confusion matrix:\\n', c)\n",
    "print('sensitivity', c[0, 0] / (c[0, 1] + c[0, 0]))\n",
    "print('specificity', c[1, 1] / (c[1, 1] + c[1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 50, 50, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 25, 25, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 25, 25, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 13, 13, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 13, 13, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 63,659,322\n",
      "Trainable params: 63,659,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7268 samples, validate on 808 samples\n",
      "Epoch 1/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.6607 - acc: 0.6054 - val_loss: 0.6488 - val_acc: 0.5693\n",
      "Epoch 2/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.6026 - acc: 0.7195 - val_loss: 0.5706 - val_acc: 0.7488\n",
      "Epoch 3/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.5082 - acc: 0.8111 - val_loss: 0.4631 - val_acc: 0.8057\n",
      "Epoch 4/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.3909 - acc: 0.8641 - val_loss: 0.3636 - val_acc: 0.8738\n",
      "Epoch 5/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.3097 - acc: 0.8923 - val_loss: 0.3018 - val_acc: 0.9022\n",
      "Epoch 6/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2690 - acc: 0.9037 - val_loss: 0.2744 - val_acc: 0.9109\n",
      "Epoch 7/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2477 - acc: 0.9128 - val_loss: 0.2623 - val_acc: 0.9121\n",
      "Epoch 8/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2338 - acc: 0.9185 - val_loss: 0.2515 - val_acc: 0.9146\n",
      "Epoch 9/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2257 - acc: 0.9218 - val_loss: 0.2409 - val_acc: 0.9158\n",
      "Epoch 10/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2163 - acc: 0.9269 - val_loss: 0.2329 - val_acc: 0.9233\n",
      "Epoch 11/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2114 - acc: 0.9293 - val_loss: 0.2170 - val_acc: 0.9220\n",
      "Epoch 12/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.2020 - acc: 0.9319 - val_loss: 0.2114 - val_acc: 0.9245\n",
      "Epoch 13/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1990 - acc: 0.9349 - val_loss: 0.2096 - val_acc: 0.9319\n",
      "Epoch 14/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1970 - acc: 0.9355 - val_loss: 0.2074 - val_acc: 0.9332\n",
      "Epoch 15/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1928 - acc: 0.9381 - val_loss: 0.1996 - val_acc: 0.9332\n",
      "Epoch 16/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1886 - acc: 0.9388 - val_loss: 0.1990 - val_acc: 0.9369\n",
      "Epoch 17/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1876 - acc: 0.9381 - val_loss: 0.1945 - val_acc: 0.9282\n",
      "Epoch 18/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1836 - acc: 0.9424 - val_loss: 0.1906 - val_acc: 0.9369\n",
      "Epoch 19/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1800 - acc: 0.9424 - val_loss: 0.1901 - val_acc: 0.9319\n",
      "Epoch 20/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1790 - acc: 0.9426 - val_loss: 0.1894 - val_acc: 0.9369\n",
      "Epoch 21/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1787 - acc: 0.9436 - val_loss: 0.1881 - val_acc: 0.9369\n",
      "Epoch 22/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1754 - acc: 0.9414 - val_loss: 0.1864 - val_acc: 0.9406\n",
      "Epoch 23/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1729 - acc: 0.9439 - val_loss: 0.1921 - val_acc: 0.9431\n",
      "Epoch 24/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1726 - acc: 0.9436 - val_loss: 0.1824 - val_acc: 0.9406\n",
      "Epoch 25/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1691 - acc: 0.9452 - val_loss: 0.1818 - val_acc: 0.9443\n",
      "Epoch 26/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1662 - acc: 0.9469 - val_loss: 0.1814 - val_acc: 0.9295\n",
      "Epoch 27/50\n",
      "7268/7268 [==============================] - 9s 1ms/sample - loss: 0.1656 - acc: 0.9466 - val_loss: 0.1872 - val_acc: 0.9443\n"
     ]
    }
   ],
   "source": [
    "loss1 = []\n",
    "val_loss1 =[]\n",
    "param1 = []\n",
    "activ = 'tanh'\n",
    "model, param1 = VGG_cifar100(activ)\n",
    "\n",
    "loss1, val_loss1, hist1= train_data(model, loss1, val_loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7268 samples, validate on 808 samples\n",
      "Epoch 1/50\n",
      "7268/7268 [==============================] - 3s 408us/sample - loss: 0.7768 - acc: 0.4600 - mean_absolute_error: 0.5133 - val_loss: 0.6988 - val_acc: 0.4963 - val_mean_absolute_error: 0.5004\n",
      "Epoch 2/50\n",
      "7268/7268 [==============================] - 2s 272us/sample - loss: 0.6841 - acc: 0.5976 - mean_absolute_error: 0.4937 - val_loss: 0.6761 - val_acc: 0.6077 - val_mean_absolute_error: 0.4896\n",
      "Epoch 3/50\n",
      "7268/7268 [==============================] - 2s 275us/sample - loss: 0.6730 - acc: 0.6054 - mean_absolute_error: 0.4874 - val_loss: 0.6701 - val_acc: 0.5829 - val_mean_absolute_error: 0.4850\n",
      "Epoch 4/50\n",
      "7268/7268 [==============================] - 2s 276us/sample - loss: 0.6587 - acc: 0.6286 - mean_absolute_error: 0.4787 - val_loss: 0.6519 - val_acc: 0.6300 - val_mean_absolute_error: 0.4738\n",
      "Epoch 5/50\n",
      "7268/7268 [==============================] - 2s 280us/sample - loss: 0.6495 - acc: 0.6439 - mean_absolute_error: 0.4716 - val_loss: 0.6453 - val_acc: 0.6040 - val_mean_absolute_error: 0.4665\n",
      "Epoch 6/50\n",
      "7268/7268 [==============================] - 2s 274us/sample - loss: 0.6286 - acc: 0.6790 - mean_absolute_error: 0.4592 - val_loss: 0.6218 - val_acc: 0.6720 - val_mean_absolute_error: 0.4540\n",
      "Epoch 7/50\n",
      "7268/7268 [==============================] - 2s 276us/sample - loss: 0.6098 - acc: 0.7047 - mean_absolute_error: 0.4461 - val_loss: 0.6120 - val_acc: 0.6522 - val_mean_absolute_error: 0.4416\n",
      "Epoch 8/50\n",
      "7268/7268 [==============================] - 2s 275us/sample - loss: 0.5907 - acc: 0.7221 - mean_absolute_error: 0.4326 - val_loss: 0.5821 - val_acc: 0.7042 - val_mean_absolute_error: 0.4246\n",
      "Epoch 9/50\n",
      "7268/7268 [==============================] - 2s 298us/sample - loss: 0.5671 - acc: 0.7448 - mean_absolute_error: 0.4177 - val_loss: 0.5710 - val_acc: 0.7673 - val_mean_absolute_error: 0.4236\n",
      "Epoch 10/50\n",
      "7268/7268 [==============================] - 2s 295us/sample - loss: 0.5509 - acc: 0.7562 - mean_absolute_error: 0.4043 - val_loss: 0.5360 - val_acc: 0.7859 - val_mean_absolute_error: 0.4005\n",
      "Epoch 11/50\n",
      "7268/7268 [==============================] - 2s 343us/sample - loss: 0.5444 - acc: 0.7453 - mean_absolute_error: 0.3951 - val_loss: 0.5550 - val_acc: 0.7005 - val_mean_absolute_error: 0.3877\n",
      "Epoch 12/50\n",
      "7268/7268 [==============================] - 2s 335us/sample - loss: 0.5171 - acc: 0.7711 - mean_absolute_error: 0.3798 - val_loss: 0.5032 - val_acc: 0.8007 - val_mean_absolute_error: 0.3761\n",
      "Epoch 13/50\n",
      "7268/7268 [==============================] - 2s 319us/sample - loss: 0.4944 - acc: 0.7997 - mean_absolute_error: 0.3660 - val_loss: 0.4921 - val_acc: 0.7859 - val_mean_absolute_error: 0.3610\n",
      "Epoch 14/50\n",
      "7268/7268 [==============================] - 2s 327us/sample - loss: 0.4840 - acc: 0.8045 - mean_absolute_error: 0.3573 - val_loss: 0.4893 - val_acc: 0.7525 - val_mean_absolute_error: 0.3508\n",
      "Epoch 15/50\n",
      "7268/7268 [==============================] - 2s 277us/sample - loss: 0.4751 - acc: 0.8057 - mean_absolute_error: 0.3480 - val_loss: 0.4564 - val_acc: 0.8428 - val_mean_absolute_error: 0.3435\n",
      "Epoch 16/50\n",
      "7268/7268 [==============================] - 2s 275us/sample - loss: 0.4505 - acc: 0.8304 - mean_absolute_error: 0.3347 - val_loss: 0.4553 - val_acc: 0.7884 - val_mean_absolute_error: 0.3314\n",
      "Epoch 17/50\n",
      "7268/7268 [==============================] - 2s 274us/sample - loss: 0.4400 - acc: 0.8363 - mean_absolute_error: 0.3270 - val_loss: 0.4301 - val_acc: 0.8243 - val_mean_absolute_error: 0.3182\n",
      "Epoch 18/50\n",
      "7268/7268 [==============================] - 2s 276us/sample - loss: 0.4372 - acc: 0.8279 - mean_absolute_error: 0.3198 - val_loss: 0.5029 - val_acc: 0.7599 - val_mean_absolute_error: 0.3581\n",
      "Epoch 19/50\n",
      "7268/7268 [==============================] - 2s 273us/sample - loss: 0.4239 - acc: 0.8350 - mean_absolute_error: 0.3109 - val_loss: 0.4528 - val_acc: 0.7599 - val_mean_absolute_error: 0.3122\n",
      "Epoch 20/50\n",
      "7268/7268 [==============================] - 2s 281us/sample - loss: 0.4090 - acc: 0.8477 - mean_absolute_error: 0.3015 - val_loss: 0.3893 - val_acc: 0.8688 - val_mean_absolute_error: 0.2944\n",
      "Epoch 21/50\n",
      "7268/7268 [==============================] - 2s 274us/sample - loss: 0.4033 - acc: 0.8487 - mean_absolute_error: 0.2958 - val_loss: 0.3892 - val_acc: 0.8391 - val_mean_absolute_error: 0.2861\n",
      "Epoch 22/50\n",
      "7268/7268 [==============================] - 2s 273us/sample - loss: 0.4002 - acc: 0.8491 - mean_absolute_error: 0.2900 - val_loss: 0.3892 - val_acc: 0.8230 - val_mean_absolute_error: 0.2814\n",
      "Epoch 23/50\n",
      "7268/7268 [==============================] - 2s 273us/sample - loss: 0.3794 - acc: 0.8634 - mean_absolute_error: 0.2784 - val_loss: 0.3654 - val_acc: 0.8577 - val_mean_absolute_error: 0.2700\n",
      "Epoch 24/50\n",
      "7268/7268 [==============================] - 2s 276us/sample - loss: 0.3879 - acc: 0.8462 - mean_absolute_error: 0.2778 - val_loss: 0.3523 - val_acc: 0.8787 - val_mean_absolute_error: 0.2629\n",
      "Epoch 25/50\n",
      "7268/7268 [==============================] - 2s 274us/sample - loss: 0.3813 - acc: 0.8492 - mean_absolute_error: 0.2726 - val_loss: 0.3528 - val_acc: 0.8577 - val_mean_absolute_error: 0.2583\n",
      "Epoch 26/50\n",
      "7268/7268 [==============================] - 2s 273us/sample - loss: 0.3687 - acc: 0.8620 - mean_absolute_error: 0.2653 - val_loss: 0.3369 - val_acc: 0.8874 - val_mean_absolute_error: 0.2524\n",
      "Epoch 27/50\n",
      "7268/7268 [==============================] - 2s 277us/sample - loss: 0.3541 - acc: 0.8766 - mean_absolute_error: 0.2571 - val_loss: 0.3345 - val_acc: 0.8861 - val_mean_absolute_error: 0.2483\n",
      "Epoch 28/50\n",
      "7268/7268 [==============================] - 2s 275us/sample - loss: 0.3441 - acc: 0.8825 - mean_absolute_error: 0.2505 - val_loss: 0.3273 - val_acc: 0.9084 - val_mean_absolute_error: 0.2470\n",
      "Epoch 29/50\n",
      "7268/7268 [==============================] - 2s 276us/sample - loss: 0.3470 - acc: 0.8736 - mean_absolute_error: 0.2493 - val_loss: 0.3194 - val_acc: 0.8923 - val_mean_absolute_error: 0.2376\n",
      "Epoch 30/50\n",
      "7268/7268 [==============================] - 2s 276us/sample - loss: 0.3353 - acc: 0.8828 - mean_absolute_error: 0.2421 - val_loss: 0.3839 - val_acc: 0.8490 - val_mean_absolute_error: 0.2739\n",
      "Epoch 31/50\n",
      "7268/7268 [==============================] - 2s 275us/sample - loss: 0.3397 - acc: 0.8762 - mean_absolute_error: 0.2416 - val_loss: 0.3123 - val_acc: 0.8849 - val_mean_absolute_error: 0.2300\n",
      "Epoch 32/50\n",
      "7268/7268 [==============================] - 2s 276us/sample - loss: 0.3308 - acc: 0.8839 - mean_absolute_error: 0.2363 - val_loss: 0.3090 - val_acc: 0.8824 - val_mean_absolute_error: 0.2257\n",
      "Epoch 33/50\n",
      "7268/7268 [==============================] - 2s 276us/sample - loss: 0.3244 - acc: 0.8865 - mean_absolute_error: 0.2319 - val_loss: 0.3168 - val_acc: 0.8589 - val_mean_absolute_error: 0.2245\n",
      "Epoch 34/50\n",
      "7268/7268 [==============================] - 2s 274us/sample - loss: 0.3219 - acc: 0.8883 - mean_absolute_error: 0.2276 - val_loss: 0.2956 - val_acc: 0.9121 - val_mean_absolute_error: 0.2189\n",
      "Epoch 35/50\n",
      "7268/7268 [==============================] - 2s 277us/sample - loss: 0.3349 - acc: 0.8769 - mean_absolute_error: 0.2312 - val_loss: 0.3023 - val_acc: 0.8738 - val_mean_absolute_error: 0.2155\n",
      "Epoch 36/50\n",
      "7268/7268 [==============================] - 2s 275us/sample - loss: 0.3166 - acc: 0.8887 - mean_absolute_error: 0.2218 - val_loss: 0.3198 - val_acc: 0.9010 - val_mean_absolute_error: 0.2321\n",
      "Epoch 37/50\n",
      "7268/7268 [==============================] - 2s 277us/sample - loss: 0.3028 - acc: 0.8978 - mean_absolute_error: 0.2148 - val_loss: 0.2995 - val_acc: 0.8812 - val_mean_absolute_error: 0.2106\n"
     ]
    }
   ],
   "source": [
    "loss1 = []\n",
    "val_loss1 =[]\n",
    "param1 = []\n",
    "activ = 'relu'\n",
    "model, param1 = dnnmodel(15, activ, param1)\n",
    "loss1, val_loss1, hist1= train_data(model, loss1, val_loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def resnet():\n",
    "    input_tensor = tf.keras.layers.Input(shape=(100, 100, 3))\n",
    "    model = ResNet50(include_top=True, weights=None, input_tensor=input_tensor, input_shape=None, pooling=None, classes=2)\n",
    "    param = model.count_params()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.00001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100, 100, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 106, 106, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 50, 50, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalizationV1) (None, 50, 50, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 50, 50, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 52, 52, 64)   0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 25, 25, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 25, 25, 64)   4160        max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 25, 25, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 25, 25, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 25, 25, 64)   36928       activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 25, 25, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 25, 25, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 25, 25, 256)  16640       activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 25, 25, 256)  16640       max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 25, 25, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 25, 25, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 25, 25, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 25, 25, 256)  0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 25, 25, 64)   16448       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 25, 25, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 25, 25, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 25, 25, 64)   36928       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 25, 25, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 25, 25, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 25, 25, 256)  16640       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 25, 25, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 25, 25, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 25, 25, 256)  0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 25, 25, 64)   16448       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 25, 25, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 25, 25, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 25, 25, 64)   36928       activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 25, 25, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 25, 25, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 25, 25, 256)  16640       activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 25, 25, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 25, 25, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 25, 25, 256)  0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 13, 13, 128)  32896       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 13, 13, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 13, 13, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 13, 13, 128)  147584      activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 13, 13, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 13, 13, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 13, 13, 512)  66048       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 13, 13, 512)  131584      activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 13, 13, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 13, 13, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 13, 13, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 13, 13, 512)  0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 13, 13, 128)  65664       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 13, 13, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 13, 13, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 13, 13, 128)  147584      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 13, 13, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 13, 13, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 13, 13, 512)  66048       activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 13, 13, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 13, 13, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 13, 13, 512)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 13, 13, 128)  65664       activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 13, 13, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 13, 13, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 13, 13, 128)  147584      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 13, 13, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 13, 13, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 13, 13, 512)  66048       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 13, 13, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 13, 13, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 13, 13, 512)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 13, 13, 128)  65664       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 13, 13, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 13, 13, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 13, 13, 128)  147584      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 13, 13, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 13, 13, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 13, 13, 512)  66048       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 13, 13, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 13, 13, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 13, 13, 512)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 7, 7, 256)    131328      activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 7, 7, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 7, 7, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 7, 7, 256)    590080      activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 7, 7, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 7, 7, 256)    0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 7, 7, 1024)   263168      activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 7, 7, 1024)   525312      activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 7, 7, 1024)   4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 7, 7, 1024)   4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 7, 7, 1024)   0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 7, 7, 1024)   0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 7, 7, 256)    262400      activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 7, 7, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 7, 7, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 7, 7, 256)    590080      activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 7, 7, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 7, 7, 256)    0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 7, 7, 1024)   263168      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 7, 7, 1024)   4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 7, 7, 1024)   0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 7, 7, 1024)   0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 7, 7, 256)    262400      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 7, 7, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 7, 7, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 7, 7, 256)    590080      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 7, 7, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 7, 7, 256)    0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 7, 7, 1024)   263168      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 7, 7, 1024)   4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 7, 7, 1024)   0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 7, 7, 1024)   0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 7, 7, 256)    262400      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 7, 7, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 7, 7, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 7, 7, 256)    590080      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 7, 7, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 7, 7, 256)    0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 7, 7, 1024)   263168      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 7, 7, 1024)   4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 7, 7, 1024)   0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 7, 7, 1024)   0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 7, 7, 256)    262400      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 7, 7, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 7, 7, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 7, 7, 256)    590080      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 7, 7, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 7, 7, 256)    0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 7, 7, 1024)   263168      activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 7, 7, 1024)   4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 7, 7, 1024)   0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 7, 7, 1024)   0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 7, 7, 256)    262400      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 7, 7, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 7, 7, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 7, 7, 256)    590080      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 7, 7, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 7, 7, 256)    0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 7, 7, 1024)   263168      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 7, 7, 1024)   4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 7, 7, 1024)   0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 7, 7, 1024)   0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 4, 4, 512)    524800      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 4, 4, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 4, 4, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 4, 4, 2048)   2099200     activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 4, 4, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 4, 4, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 4, 4, 2048)   0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 4, 4, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 4, 4, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 4, 4, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 4, 4, 2048)   0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 4, 4, 512)    1049088     activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 4, 4, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 4, 4, 512)    2359808     activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 4, 4, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 4, 4, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 4, 4, 2048)   1050624     activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 4, 4, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 4, 4, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 4, 4, 2048)   0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "fc1000 (Dense)                  (None, 2)            4098        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 23,538,690\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "Train on 7268 samples, validate on 808 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7268/7268 [==============================] - 22s 3ms/sample - loss: 0.6956 - acc: 0.5557 - val_loss: 3.2630 - val_acc: 0.5656\n",
      "Epoch 2/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.6496 - acc: 0.6095 - val_loss: 0.7184 - val_acc: 0.6089\n",
      "Epoch 3/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.6306 - acc: 0.6379 - val_loss: 0.6427 - val_acc: 0.6250\n",
      "Epoch 4/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.6004 - acc: 0.6783 - val_loss: 0.6067 - val_acc: 0.6708\n",
      "Epoch 5/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.5615 - acc: 0.7133 - val_loss: 0.5849 - val_acc: 0.6918\n",
      "Epoch 6/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.5200 - acc: 0.7367 - val_loss: 0.5430 - val_acc: 0.7401\n",
      "Epoch 7/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.4868 - acc: 0.7577 - val_loss: 0.5114 - val_acc: 0.7587\n",
      "Epoch 8/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.4397 - acc: 0.7980 - val_loss: 0.4744 - val_acc: 0.7723\n",
      "Epoch 9/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.4047 - acc: 0.8182 - val_loss: 0.5008 - val_acc: 0.7797\n",
      "Epoch 10/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.3855 - acc: 0.8221 - val_loss: 0.4458 - val_acc: 0.8168\n",
      "Epoch 11/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.3505 - acc: 0.8473 - val_loss: 0.4353 - val_acc: 0.8193\n",
      "Epoch 12/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.3434 - acc: 0.8476 - val_loss: 0.4386 - val_acc: 0.8267\n",
      "Epoch 13/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.3160 - acc: 0.8636 - val_loss: 0.3970 - val_acc: 0.8304\n",
      "Epoch 14/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.3030 - acc: 0.8693 - val_loss: 0.4281 - val_acc: 0.8267\n",
      "Epoch 15/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2959 - acc: 0.8697 - val_loss: 0.4337 - val_acc: 0.8193\n",
      "Epoch 16/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2811 - acc: 0.8771 - val_loss: 0.3813 - val_acc: 0.8329\n",
      "Epoch 17/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2600 - acc: 0.8930 - val_loss: 0.4352 - val_acc: 0.8453\n",
      "Epoch 18/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2462 - acc: 0.8961 - val_loss: 0.4062 - val_acc: 0.8366\n",
      "Epoch 19/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2444 - acc: 0.8987 - val_loss: 0.3429 - val_acc: 0.8676\n",
      "Epoch 20/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2366 - acc: 0.9037 - val_loss: 0.3503 - val_acc: 0.8564\n",
      "Epoch 21/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2217 - acc: 0.9111 - val_loss: 0.3481 - val_acc: 0.8552\n",
      "Epoch 22/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2187 - acc: 0.9119 - val_loss: 0.3283 - val_acc: 0.8787\n",
      "Epoch 23/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.2051 - acc: 0.9172 - val_loss: 0.3462 - val_acc: 0.8750\n",
      "Epoch 24/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.1936 - acc: 0.9256 - val_loss: 0.3858 - val_acc: 0.8577\n",
      "Epoch 25/50\n",
      "7268/7268 [==============================] - 16s 2ms/sample - loss: 0.1907 - acc: 0.9229 - val_loss: 0.3503 - val_acc: 0.8626\n"
     ]
    }
   ],
   "source": [
    "loss1 = []\n",
    "val_loss1 =[]\n",
    "param1 = []\n",
    "model, param = resnet()\n",
    "loss1, val_loss1, hist1= train_data(model, loss1, val_loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_3_channel(X):\n",
    "    new = []\n",
    "    for image in X:\n",
    "        r = image[:,:,0]\n",
    "        g = image[:,:,1]\n",
    "        b = image[:,:,2]\n",
    "        add = r+g+b\n",
    "        new.append(add)\n",
    "    return np.array(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = add_3_channel(X_train)\n",
    "X_val = add_3_channel(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CNN used in class\n",
    "def VGG_gray(activ):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=activ, input_shape=(100, 100)),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(128, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(256, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.Conv2D(256, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "\n",
    "        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.Conv2D(512, (3,3), padding='same', activation=activ),\n",
    "        tf.keras.layers.MaxPool2D(padding='same'),\n",
    "\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation=activ),\n",
    "        tf.keras.layers.Dense(4096, activation=activ),\n",
    "        tf.keras.layers.Dense(1000, activation=activ),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    param = model.count_params()\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.000001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "   \n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = []\n",
    "val_loss1 =[]\n",
    "param1 = []\n",
    "activ = 'relu'\n",
    "model, param1 = VGG_gray(activ)\n",
    "loss1, val_loss1, hist1= train_data(model, loss1, val_loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulating a microscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast as a float32:\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the dataset into microscope samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelength = .5\n",
    "thickness = 1 * wavelength\n",
    "\n",
    "def convert_dataset(X, wavelength, thickness):\n",
    "    X1 = X / np.max(X)\n",
    "    X2 = np.exp(1j * X1 * thickness / wavelength)\n",
    "    return X2\n",
    "\n",
    "X_train = convert_dataset(X_train, wavelength, thickness)\n",
    "X_val = convert_dataset(X_val, wavelength, thickness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create input pipeline for generating training/testing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be either X_train/y_train or X_test/y_test, so we make a placeholder that we can feed into:\n",
    "X_train_or_test = tf.placeholder(tf.complex64, [3, 100, 100], name='input_image')\n",
    "y_train_or_test = tf.placeholder(tf.int32, [None], name='image_label')\n",
    "batch_size = 32\n",
    "\n",
    "# create a tf dataset, from which we can generate batches\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train_or_test, y_train_or_test))\n",
    "dataset = dataset.batch(batch_size).repeat(None)\n",
    "batch_generator = dataset.make_initializable_iterator()\n",
    "X_batch, y_batch = batch_generator.get_next()  # batches symbolically generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create complex-valued trainable illumination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this flag to allow/disallow training of the input illumination; tf.Variable has an argument called \"trainable\":\n",
    "train_illumination = True  \n",
    "\n",
    "# create the variable corresponding to the input illumination phase; initialize to a constant phase:\n",
    "# (remember this is a weight variable that you will optimize!)\n",
    "input_illumination_phase = tf.Variable(tf.random_uniform([100,100]),dtype=np.float32,trainable=train_illumination)\n",
    "input_illumination_phase = tf.cast(input_illumination_phase, tf.complex64)\n",
    "\n",
    "# using that input phase, create the input field:\n",
    "input_illumination = tf.exp(1j*input_illumination_phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the emerging field from the sample and propagate the emerging field to the aperture plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be sure to match the shapes/dimensions to enable broadcasting:\n",
    "emerging_field = X_batch * input_illumination\n",
    "aperture_plane = tf.fft2d(emerging_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create complex-valued trainable aperture function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use this flag to allow/disallow training of the aperture plane; pass this into the tf.Variable definition:\n",
    "train_aperture = True\n",
    "\n",
    "# the aperture function consists of two parts: 1) the circular aperture as in HW4, and 2) a trainable 28x28 phase array\n",
    "# create a circular aperture as you did in HW4:\n",
    "r = 0.25 * 100\n",
    "circ_aper1 = np.zeros([100,100])\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        if (i-50)**2+(j-50)**2 <= r**2:\n",
    "            circ_aper1[i,j] = 1\n",
    "\n",
    "plt.imshow(circ_aper1)\n",
    "circ_aper2 = np.dstack([circ_aper1,circ_aper1,circ_aper1])\n",
    "circ_aper = np.fft.fftshift(circ_aper1)\n",
    "\n",
    "# create the variable corresponding to the aperture phase; initialize to a constant phase:\n",
    "# (remember this is a weight variable that you will optimize!)\n",
    "aperture_phase = tf.Variable(tf.random_uniform([100,100]),dtype=np.float32)\n",
    "aperture_phase = tf.cast(aperture_phase, tf.complex64)\n",
    "\n",
    "# write the full aperture function, combining the above two components:\n",
    "aperture = circ_aper * tf.exp(1j * aperture_phase)\n",
    "\n",
    "# filter the field that you propagated:\n",
    "aperture_plane_filtered = aperture_plane * aperture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### propagate to the image plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propagate the field from the aperture plane to the image plane and convert it to intensity:\n",
    "image = (tf.abs(tf.ifft2d(aperture_plane_filtered)))**2\n",
    "#image = tf.abs(tf_fftshift2(tf.fft2d(tf_ifftshift2(aperture_plane_filtered))))**2\n",
    "\n",
    "# add noise:\n",
    "#image += tf.random_normal([100,100],stddev=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process the simulated image through a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "net = image[..., None]  # add a channels dimension\n",
    "\n",
    "# define your CNN here\n",
    "\n",
    "# add some convolutional layers:\n",
    "net = tf.layers.conv2d(net, filters=32, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=32, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "net = tf.layers.max_pooling2d(net, pool_size=2, strides=2)\n",
    "\n",
    "# add some more if you want:\n",
    "net = tf.layers.conv2d(net, filters=64, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "net = tf.layers.conv2d(net, filters=64, kernel_size=3, padding='SAME', activation=tf.nn.relu)\n",
    "net = tf.layers.max_pooling2d(net, pool_size=2, strides=2)\n",
    "\n",
    "# fully connected layers:\n",
    "net = tf.layers.flatten(net)\n",
    "net = tf.layers.dense(net, units=512, activation=tf.nn.relu)\n",
    "net = tf.layers.flatten(net)\n",
    "net = tf.layers.dense(net, units=10)\n",
    "\n",
    "logits = net\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(y_batch, depth=10), logits=logits)\n",
    "\n",
    "# boilerplate code:\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=.01).minimize(loss)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (7268, 100, 100, 3) for Tensor 'input_image_3:0', which has shape '(3, 100, 100)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-f4fd600bf470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# this code tells our batch_generator to generate training batches:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX_train_or_test\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_or_test\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1074\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1076\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1077\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (7268, 100, 100, 3) for Tensor 'input_image_3:0', which has shape '(3, 100, 100)'"
     ]
    }
   ],
   "source": [
    "# this code tells our batch_generator to generate training batches:\n",
    "sess.run(batch_generator.initializer, feed_dict={X_train_or_test: X_train, y_train_or_test: y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(batch):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(batch[i])\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plot_examples(image.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop:\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(10000):\n",
    "    _, loss_i, prediction, truth = sess.run([train_op, loss, logits, y_batch])\n",
    "    if i%1000 == 0:\n",
    "        print(loss_i)\n",
    "    correct += np.sum(prediction.argmax(1)==truth)\n",
    "    total += len(truth)\n",
    "acc = correct/total\n",
    "print(\"Accuracy is:\")\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot examples \n",
    "plot_examples(image.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code tells our batch_generator to generate test batches:\n",
    "sess.run(batch_generator.initializer, feed_dict={X_train_or_test: X_test, y_train_or_test: y_test})\n",
    "\n",
    "# pass through test set:\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(100):\n",
    "    prediction, truth = sess.run([logits, y_batch])\n",
    "    correct += np.sum(prediction.argmax(1)==truth)\n",
    "    total += len(truth)\n",
    "acc = correct/total\n",
    "print(\"Test accuracy is:\")\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
